{"cells":[{"cell_type":"code","execution_count":null,"id":"83902367","metadata":{},"outputs":[],"source":["!gcloud dataproc clusters list --region us-central1"]},{"cell_type":"markdown","id":"911ae7db","metadata":{},"source":["# Imports & Setup"]},{"cell_type":"code","execution_count":null,"id":"685a351b","metadata":{},"outputs":[],"source":["!pip install -q google-cloud-storage==1.43.0\n","!pip install -q graphframes"]},{"cell_type":"code","execution_count":null,"id":"ec2fd91b","metadata":{},"outputs":[],"source":["import pyspark\n","import sys\n","from collections import Counter, OrderedDict, defaultdict\n","import itertools\n","from itertools import islice, count, groupby\n","import pandas as pd\n","import os\n","import re\n","from operator import itemgetter\n","import nltk\n","from nltk.stem.porter import *\n","from nltk.corpus import stopwords\n","from time import time\n","from pathlib import Path\n","import pickle\n","import pandas as pd\n","from google.cloud import storage\n","from collections import Counter\n","from nltk.corpus.reader.rte import norm\n","\n","import hashlib\n","def _hash(s):\n","    return hashlib.blake2b(bytes(s, encoding='utf8'), digest_size=5).hexdigest()\n","\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"id":"bf630626","metadata":{},"outputs":[],"source":["!ls -l /usr/lib/spark/jars/graph*"]},{"cell_type":"code","execution_count":null,"id":"0024a733","metadata":{},"outputs":[],"source":["from pyspark.sql import *\n","from pyspark.sql.functions import *\n","from pyspark import SparkContext, SparkConf, SparkFiles\n","from pyspark.sql import SQLContext\n","from graphframes import *"]},{"cell_type":"markdown","id":"4ad4d658","metadata":{},"source":["# Load the data"]},{"cell_type":"code","execution_count":null,"id":"ad0f3197","metadata":{},"outputs":[],"source":["full_path = \"gs://wikidata_preprocessed/*\"\n","parquetFile = spark.read.parquet(full_path)\n","\n","doc_body_pairs = parquetFile.select(\"text\", \"id\").rdd\n","doc_title_pairs = parquetFile.select(\"title\", \"id\").rdd\n","doc_anchor_pairs = parquetFile.select(\"anchor_text\", \"id\").rdd"]},{"cell_type":"code","execution_count":null,"id":"e0c80570","metadata":{},"outputs":[],"source":["%cd -q /home/dataproc\n","!ls inverted_index_gcp.py"]},{"cell_type":"code","execution_count":null,"id":"42bb5e8f","metadata":{},"outputs":[],"source":["sc.addFile(\"/home/dataproc/inverted_index_gcp.py\")\n","sys.path.insert(0,SparkFiles.getRootDirectory())"]},{"cell_type":"code","execution_count":null,"id":"1b687aad","metadata":{},"outputs":[],"source":["from inverted_index_gcp import InvertedIndex\n","from inverted_index_gcp import MultiFileReader"]},{"cell_type":"markdown","id":"2aaf718d","metadata":{},"source":["# Preproc"]},{"cell_type":"code","execution_count":null,"id":"05cf6d2c","metadata":{},"outputs":[],"source":["english_stopwords = frozenset(stopwords.words('english'))\n","\n","\n","def doc_count(id,text):\n"," counter = Counter()\n"," tokens = tokeniz_clean(text)\n"," result = []\n"," for token in tokens:\n","  counter[token] += 1\n"," for count in counter : \n","  result.append((id,(count,counter[count])))\n"," return result\n","\n","\n","\n","def tokeniz_clean(text):\n","  list_token = []\n","  RE_WORD = re.compile(r\"\"\"[\\#\\@\\w](['\\-]?\\w){2,24}\"\"\", re.UNICODE)\n","  stemmer = PorterStemmer()\n","  tokens = [token.group() for token in RE_WORD.finditer(text.lower())]\n","  for token in tokens:\n","    if token not in english_stopwords: \n","      list_token.append(stemmer.stem(token))\n","  return list_token\n","\n","def word_count(text, id):\n","  ''' Count the frequency of each word in `text` (tf) that is not included in \n","  `all_stopwords` and return entries that will go into our posting lists. \n","  Parameters:\n","  -----------\n","    text: str\n","      Text of one document\n","    id: int\n","      Document id\n","  Returns:\n","  --------\n","    List of tuples\n","      A list of (token, (doc_id, tf)) pairs \n","      for example: [(\"Anarchism\", (12, 5)), ...]\n","  '''\n","\n","  stemmer = PorterStemmer()\n","  counter = Counter()\n","  tokens = tokeniz_clean(text)\n","  result = []\n","  for token in tokens:\n","    if token not in english_stopwords: \n","      counter[stemmer.stem(token)] += 1\n","  for count in counter : \n","    result.append((count,(id,counter[count])))\n","  return result\n","\n","def len_doc(text):\n","  count = 0\n","  tokens =  tokeniz_clean(text)\n","  for token in tokens:\n","      count+=1\n","  return count\n","  \n","def word_count2(text, id):\n","\n","  counter = Counter()\n","  tokens = tokeniz_clean(text)\n","  result = []\n","  for i,token in enumerate(tokens):\n","    if len(tokens)-1 == i: break\n","    counter[tokens[i] +\" \"+ tokens[i+1]] += 1\n","\n","  for count in counter : \n","    result.append((count,(id,counter[count])))\n","  return result\n","\n","import math \n","\n","def reduce_word_counts(unsorted_pl):\n","\n","  return sorted(unsorted_pl,key= lambda x:x[1],reverse = True)\n","\n","def calculate_norm(norm1):\n","  count=0\n","  doc_norm = {}\n","  for doc,frek in norm1.items():\n","    count = 0\n","    for frek_t in frek:\n","      count += frek_t[1]**2\n","    doc_norm[doc] = 1/math.sqrt(count)\n","  return doc_norm\n","\n","def calculate_df(postings):\n","  return postings.map(lambda X1: (X1[0],len(X1[1])))\n","\n"]},{"cell_type":"code","execution_count":null,"id":"eae0f6a0","metadata":{},"outputs":[],"source":["def token2bucket_id(token,num_buckets):\n","  return int(_hash(token),16) % num_buckets\n","\n","def write_bucket(posting,bucket_name,num_buckets):\n","  ''' A function that partitions the posting lists into buckets, writes out \n","  all posting lists in a bucket to disk, and merge the posting locations into a single dict,\n","  Finally create the index.\n","\n","  Parameters:\n","  -----------\n","    postings: RDD\n","      An RDD where each item is a (w, posting_list, bucket_map) pair.\n","  Returns:\n","  --------\n","    RDD\n","      inverted index.\n","  '''\n","       \n","  tempRDD = posting.map(lambda x:(token2bucket_id(x[0],num_buckets),[(x[0],x[1])]))\n","  ldRDD = tempRDD.reduceByKey(lambda a,b: a+b)\n","  index = InvertedIndex()\n","  posting_list =ldRDD.map(lambda x: index.write_a_posting_list(x,bucket_name))\n","  return posting_list\n","\n","def create_df_posting(doc_pairs,name,k,num_buckets):\n","  ''' A function that count the word in each doc, group them together and create the df.\n","       \n","      \n","  Parameters:\n","  -----------\n","    doc_pairs - RDD of doc_id,text of each doc,\n","    name - the suffix of the bucket name,\n","    k - Filter word that have less from k \n","  Returns:\n","  --------\n","    w2df - dict of df\n","    posting_filter - posting_list\n","  '''\n","    if name in [\"body2\",\"title2\"] :\n","        word_counts = doc_pairs.flatMap(lambda x: word_count2(x[0], x[1]))\n","    else:\n","        word_counts = doc_pairs.flatMap(lambda x: word_count(x[0], x[1]))\n","    postings = word_counts.groupByKey().mapValues(reduce_word_counts)\n","    postings_filtered = postings.filter(lambda x: len(x[1])>50).map(lambda x:(x[0], x[1][:150]))\n","    w2df = calculate_df(postings_filtered).collectAsMap()\n","    _ = write_bucket(postings_filtered,f\"206065989_{name}\",num_buckets).collect()\n","    return w2df,postings_filtered"]},{"cell_type":"code","execution_count":null,"id":"8f4112c6","metadata":{},"outputs":[],"source":["client = storage.Client()\n","\n","def super_post(bucket_name,df):\n","\n","    super_posting_locs = defaultdict(list)\n","    blobs = client.list_blobs(bucket_name)\n","    for blob in client.list_blobs(bucket_name, prefix='postings_gcp'):\n","      if not blob.name.endswith(\"pickle\"):\n","        continue\n","      with blob.open(\"rb\") as f:\n","        posting_locs = pickle.load(f)\n","        posting_rdd = sc.parallelize(posting_locs.items())\n","        df_rdd = sc.parallelize(df.items())\n","        posting_locs = df_rdd.join(posting_rdd).sortBy(lambda x :x[1][0], False).map(lambda x:(x[0],x[1][1])).collectAsMap()\n","        for k, v in posting_locs.items():\n","          super_posting_locs[k].extend(v)\n","    return super_posting_locs\n"]},{"cell_type":"code","execution_count":null,"id":"6b6f797b","metadata":{},"outputs":[],"source":["#Manipulation on anchor text\n","doc_anchor_pairs = doc_anchor_pairs.flatMap(lambda x:(x[0]))\n","doc_anchor_pairs_filter = doc_anchor_pairs.filter(lambda x:len(x[1])>1)\n","doc_anchor_pairs_filter = doc_anchor_pairs_filter.filter(lambda x:len((x[1].split(\" \"))[0])>1)\n","doc_anchor_pairs_filter = doc_anchor_pairs_filter.map(lambda x:(x[1],x[0]))"]},{"cell_type":"markdown","id":"b602599e","metadata":{},"source":["# Create index "]},{"cell_type":"code","execution_count":null,"id":"d2378910","metadata":{},"outputs":[],"source":["def crete_index(name):\n","    # Create inverted index instance\n","    inverted = InvertedIndex()\n","    # Adding the posting locations dictionary to the inverted index\n","    if name == \"body\":\n","        inverted.doc_len = dict_body_len.collectAsMap()\n","        inverted.num_doc = len(inverted.doc_len)\n","        inverted.df,posting_filter = create_df_posting(doc_body_pairs,\"body\",50,124)\n","        tf_idf_filtered = posting_filter.map(lambda x: (x[0],[(y[0],(y[1]/inverted.doc_len[y[0]]) * np.log2(inverted.num_doc/inverted.df[x[0]])) for y in x[1]]))\n","        inverted.norm = tf_idf_filtered.flatMap(lambda x: [(y[0],y[1]**2)  for y in x[1]]).reduceByKey(lambda x, y: x+y).collectAsMap()\n","  \n","    elif name == 'title':\n","        inverted.df,posting_filter = create_df_posting(doc_title_pairs,\"title\",0,124)\n","        inverted.title = doc_title_pairs.toDF().select(\"id\",\"title\").rdd.collectAsMap()\n","        \n","    elif name == 'anchor':\n","        inverted.df,posting_filter = create_df_posting(doc_anchor_pairs,\"anchor\",10,124)\n","        \n","    else:\n","        inverted.df,posting_filter = create_df_posting(doc_title_pairs,\"title2\",0,124)\n","        \n","    inverted.posting_locs = super_post(f\"206065989_{name}\",inverted.df)\n","\n","\n","    # write the global stats out\n","    inverted.write_index('.', f'index_{name}')\n","    # upload to gs\n","    index_src = f\"index_{name}.pkl\"\n","    index_dst = f'gs://206065989_{name}/postings_gcp/{index_src}'\n","    !gsutil cp $index_src $index_dst"]},{"cell_type":"code","execution_count":null,"id":"f951591d","metadata":{},"outputs":[],"source":["crete_index(\"body\")\n","crete_index(\"title\")\n","crete_index(\"title2\")\n","crete_index(\"anchor\")\n","crete_index(\"title_pure\")"]},{"cell_type":"markdown","id":"69fab5c6","metadata":{},"source":["# PageRank"]},{"cell_type":"code","execution_count":null,"id":"c58791c7","metadata":{},"outputs":[],"source":["def generate_graph(pages):\n","  ''' Compute the directed graph generated by wiki links.\n","  Parameters:\n","  -----------\n","    pages: RDD\n","      An RDD where each row consists of one wikipedia articles with 'id' and \n","      'anchor_text'.\n","  Returns:\n","  --------\n","    edges: RDD\n","      An RDD where each row represents an edge in the directed graph created by\n","      the wikipedia links. The first entry should the source page id and the \n","      second entry is the destination page id. No duplicates should be present. \n","    vertices: RDD\n","      An RDD where each row represents a vetrix (node) in the directed graph \n","      created by the wikipedia links. No duplicates should be present. \n","  '''\n","  edges = pages.flatMap(lambda x:map(lambda y:(x['id'],y[0]),x['anchor_text'])).distinct()\n","  vertices_s = pages.map(lambda x:x[0])\n","  anchor_text = pages.flatMap(lambda x:x[1])\n","  vertices_t = anchor_text.map(lambda x:x[0])\n","  vertices = vertices_s.union(vertices_t).distinct().map(lambda x:(x,))\n","  return edges, vertices"]},{"cell_type":"code","execution_count":null,"id":"8c90b7e9","metadata":{},"outputs":[],"source":["bucket_name = '' \n","t_start = time()\n","pages_links = spark.read.parquet(\"gs://wikidata_preprocessed/*\").select(\"id\", \"anchor_text\").rdd\n","# construct the graph \n","edges, vertices = generate_graph(pages_links)\n","# compute PageRank\n","edgesDF = edges.toDF(['src', 'dst']).repartition(124, 'src')\n","verticesDF = vertices.toDF(['id']).repartition(124, 'id')\n","g = GraphFrame(verticesDF, edgesDF)\n","pr_results = g.pageRank(resetProbability=0.15, maxIter=6)\n","pr = pr_results.vertices.select(\"id\", \"pagerank\")\n","pr = (pr.toPandas()).to_dict()\n","pr = pd.DataFrame(pr)\n","q = dict(zip(pr.id, pr.pagerank))"]},{"cell_type":"markdown","id":"1862615a","metadata":{},"source":["# Page views"]},{"cell_type":"code","execution_count":null,"id":"12cd5df8","metadata":{},"outputs":[],"source":["pv_path = 'https://dumps.wikimedia.org/other/pageview_complete/monthly/2021/2021-08/pageviews-202108-user.bz2'\n","p = Path(pv_path) \n","pv_name = p.name\n","pv_temp = f'{p.stem}-4dedup.txt'\n","pv_clean = f'{p.stem}.pkl'\n"," \n","!wget -N $pv_path\n"," \n","!bzcat $pv_name | grep \"^en\\.wikipedia\" | cut -d' ' -f3,5 | grep -P \"^\\d+\\s\\d+$\" > $pv_temp\n"," \n","wid2pv = Counter()\n","with open(pv_temp, 'rt') as f:\n","  for line in f:\n","    parts = line.split(' ')\n","    wid2pv.update({int(parts[0]): int(parts[1])})\n","# write out the counter as binary file (pickle it)\n","with open(pv_clean, 'wb') as f:\n","  pickle.dump(wid2pv, f)"]},{"cell_type":"markdown","id":"d9114a55","metadata":{},"source":["# Reporting"]},{"cell_type":"code","execution_count":null,"id":"642d87e3","metadata":{},"outputs":[],"source":["# size of index data\n","index_dst = f'gs://{bucket_name}/postings_gcp/'\n","!gsutil du -sh \"$index_dst\""]}],"metadata":{"celltoolbar":"Create Assignment","colab":{"collapsed_sections":[],"name":"assignment3_gcp.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"PySpark","language":"python","name":"pyspark"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.12"}},"nbformat":4,"nbformat_minor":5}
